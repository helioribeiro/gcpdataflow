# ğŸš€Â BigQuery âœ ApacheÂ BeamÂ (Dataflow) âœ Parquet onÂ GCS

<p align="center">
  <img alt="GCP" src="https://img.shields.io/badge/GCP-Dataflow-blue?logo=googlecloud" />
  <img alt="ApacheÂ Beam" src="https://img.shields.io/badge/Apache%20Beam-2.56-orange?logo=apache" />
  <img alt="Python" src="https://img.shields.io/badge/Python-3.10-yellow?logo=python" />
  <img alt="Terraform" src="https://img.shields.io/badge/Terraform-1.x-purple?logo=terraform" />
</p>

> **Demo**: query a BigQuery public dataset, filter it with ApacheÂ Beam,  
> and write the result as Parquet to CloudÂ Storageâ€”locally with **DirectRunner** or fullyâ€‘managed on **Dataflow**, all provisioned by **Terraform**.

---

## ğŸ“–Â TableÂ ofÂ Contents

1. [Architecture](#-architecture)
2. [QuickÂ Start](#-quick-start)
3. [Prerequisites](#-prerequisites)
4. [InfrastructureÂ asÂ Code](#-infrastructure-as-code--terraform)
5. [LocalÂ RunÂ (DirectRunner)](#-local-run-directrunner)
6. [CloudÂ RunÂ (Dataflow)](#-cloud-run-dataflow)
7. [CostÂ &Â Cleanup](#-cost--cleanup)
8. [ProjectÂ Structure](#-project-structure)

---

## ğŸ–¼ï¸Â Architecture

Dataflow (Python), BigQuery, GCS, TerraForm, Docker.

* **Dataset**: `bigquery-public-data.samples.shakespeare` (~6â€¯MB, free tier).  
* **Transform**: keep words with `word_count > 100`.  
* **Sink**: Parquet file(s) in a Terraformâ€‘generated bucket `gs://devhelio-460409-dataflow-temp-XXXX/`.

---

## âš¡Â Quick Start

```bash
# 0) clone & cd
git clone https://github.com/helioribeiro/gcpdataflow
cd gcpdataflow
gcloud services enable dataflow.googleapis.com bigquery.googleapis.com compute.googleapis.com iam.googleapis.com serviceusage.googleapis.com storage.googleapis.com

# 1) provision infra
PROJECT_ID=$(gcloud config get-value project 2>/dev/null) && \
echo "ğŸ”§  Using project: $PROJECT_ID" && \
cd terraform && \
terraform init && \
terraform apply -auto-approve -var="project_id=${PROJECT_ID}"
# grab the bucket_name and service_account_email outputs
cd ..

# 2) local smokeâ€‘test (DirectRunner)
docker compose build         
docker compose run pipeline   --output gs://$(terraform -chdir=terraform output -raw bucket_name)/output/shakespeare

# 3) launch in the cloud (DataflowRunner)
python pipeline.py   --runner DataflowRunner   --project devhelio-460409   --region us-central1   --temp_location gs://$(terraform -chdir=terraform output -raw bucket_name)/temp/   --staging_location gs://$(terraform -chdir=terraform output -raw bucket_name)/staging/   --service_account_email $(terraform -chdir=terraform output -raw service_account_email)   --output gs://$(terraform -chdir=terraform output -raw bucket_name)/output/shakespeare   --max_num_workers 1
```

ğŸ‘‰Â Check **Dataflowâ€¯â†’â€¯Jobs** for progress, and **Cloudâ€¯Storage** for the Parquet file when done.

---

## ğŸ› ï¸Â Prerequisites

| Tool | Version | Purpose |
|------|---------|---------|
| **gcloud** | â‰¥â€¯467 | Auth & API enablement |
| **Terraform** | â‰¥â€¯1.3 | IaC provisioning |
| **DockerÂ &Â Compose** | â‰¥â€¯24 | Local DirectRunner (optional) |
| **Python** | â‰¥â€¯3.10 | Direct CLI submission (if not using Docker) |

Make sure the following APIs are **enabled** in *devhelioâ€‘460409*:

```bash
gcloud services enable dataflow.googleapis.com                        bigquery.googleapis.com                        compute.googleapis.com
```

---

## â˜ï¸Â Infrastructure as Code â€“ Terraform

* **Randomized bucket**: `${project}-dataflow-temp-<suffix>` (multiâ€‘region **US**).  
* **Serviceâ€¯Account**: `dataflow-pipeline-sa@devhelio-460409.iam.gserviceaccount.com`  
  * `roles/dataflow.worker` â€¢ `roles/dataflow.developer` â€¢ `roles/storage.objectAdmin` (bucketâ€‘scoped)  
* **force_destroy** = true â†’ easy teardown.

```bash
terraform destroy -auto-approve -var="project_id=devhelio-460409"
```

---

## ğŸƒâ€â™‚ï¸Â Local Run (DirectRunner)

1. **Build**: `docker compose build`
2. **Run**:  
   ```bash
   docker compose run pipeline      --output gs://<BUCKET>/output/shakespeare      --temp_location gs://<BUCKET>/temp
   ```
3. **Validate**: `gsutil ls gs://<BUCKET>/output/`

No Dataflow charges; only a tiny BigQuery read (under free tier).

---

## â˜ï¸Â Cloud Run (Dataflow)

Submit with either **Python** (native) or **Docker**:

```bash
# native
python pipeline.py --runner DataflowRunner ...

# containerised
docker run --rm -v ~/.config/gcloud:/root/.config/gcloud:ro   your-image:latest --runner DataflowRunner ...
```

### Cost GuardrailsÂ ğŸ’¸

| Flag | Value | Why |
|------|-------|-----|
| `--max_num_workers` | **1** | Prevent autoâ€‘scaling |
| `--worker_machine_type` | `e2-small` (optional) | Cheapest vCPU |
| `--disk_size_gb` | `20` (optional) | Minimal persistent disk |
| **Data size** | 6â€¯MB read | Fits in free tier |

A single run typically finishes in <â€¯5â€¯min and costs **â‰ˆâ€¯$0.02**.

---

## ğŸ§¹Â Cost & Cleanup

| Resource | Ongoing cost | Cleanup |
|----------|--------------|---------|
| **GCS bucket** | a few MB âŸ¶ ~\$0.00 | `terraform destroy` |
| **Service account** | free | destroy |
| **Dataflow job** | perâ€‘job | autoâ€‘deleted after 30â€¯days |

Always destroy the bucket if you no longer need the output files.

---

## ğŸ“‚Â Project Structure

```
.
â”œâ”€â”€ Dockerfile              # Beam + PyArrow image
â”œâ”€â”€ docker-compose.yml      # local DirectRunner harness
â”œâ”€â”€ pipeline.py             # Apache Beam pipeline
â”œâ”€â”€ terraform/
â”‚   â””â”€â”€ main.tf             # bucket, service account, IAM
â””â”€â”€ README.md               # youâ€™re here
```

---


PROJECT_ID=$(gcloud config get-value project 2>/dev/null) && \
echo "ğŸ”§  Enabling APIs for project: $PROJECT_ID" && \
gcloud services enable iam.googleapis.com serviceusage.googleapis.com \
  --project="$PROJECT_ID" && \
echo "â³  Waiting ~30â€¯s for API propagationâ€¦" && sleep 30 && \
terraform init && \
terraform apply -auto-approve -var="project_id=$PROJECT_ID"