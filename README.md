# ğŸš€Â BigQuery âœ ApacheÂ BeamÂ (Dataflow) âœ Parquet onÂ GCS

<p align="center">
  <img alt="GCP" src="https://img.shields.io/badge/GCP-Dataflow-blue?logo=googlecloud" />
  <img alt="ApacheÂ Beam" src="https://img.shields.io/badge/Apache%20Beam-2.65-orange?logo=apache" />
  <img alt="Python" src="https://img.shields.io/badge/Python-3.12-yellow?logo=python" />
  <img alt="Terraform" src="https://img.shields.io/badge/Terraform-1.3-purple?logo=terraform" />
</p>

> **Demo**: query a BigQuery public dataset, filter it with ApacheÂ Beam,  
> and write the result as Parquet to CloudÂ Storageâ€”locally with **DirectRunner** or fullyâ€‘managed on **Dataflow**, all provisioned by **Terraform**.

---

## ğŸ“–Â TableÂ ofÂ Contents

1. [Architecture](#-architecture)
2. [QuickÂ Start](#-quick-start)
3. [Prerequisites](#-prerequisites)
4. [InfrastructureÂ asÂ Code](#-infrastructure-as-code--terraform)
5. [LocalÂ RunÂ (DirectRunner)](#-local-run-directrunner)
6. [CloudÂ RunÂ (Dataflow)](#-cloud-run-dataflow)
7. [CostÂ &Â Cleanup](#-cost--cleanup)
8. [ProjectÂ Structure](#-project-structure)

---

## ğŸ–¼ï¸Â Architecture

Dataflow (Python), BigQuery, GCS, TerraForm, Docker.

* **Dataset**: `bigquery-public-data.samples.shakespeare` (~6â€¯MB, free tier).  
* **Transform**: keep words with `word_count > 100`.  
* **Sink**: Parquet file(s) in a Terraformâ€‘generated bucket `gs://project-dataflow-temp-XXXX/`.

---

## âš¡Â Quick Start

### 1) Run the following commands
```bash
gcloud init
gcloud auth application-default login
git clone https://github.com/helioribeiro/gcpdataflow
cd gcpdataflow
gcloud services enable dataflow.googleapis.com bigquery.googleapis.com bigquerystorage.googleapis.com compute.googleapis.com iam.googleapis.com serviceusage.googleapis.com storage.googleapis.com
pip install -r requirements.txt
chmod +x scripts/create-sa-key.sh

# infra
unset GOOGLE_APPLICATION_CREDENTIALS
PROJECT_ID=$(gcloud config get-value project 2>/dev/null) && \
echo "Using project: $PROJECT_ID" && \
cd terraform && terraform init && terraform apply -auto-approve -var="project_id=$PROJECT_ID" && \
BUCKET_NAME=$(terraform output -raw bucket_name) && \
SA_EMAIL=$(terraform output -raw dataflow_service_account_email) && \
REGION=$(terraform output -raw region) && \
cd .. && \
export DATAFLOW_SA_EMAIL="$SA_EMAIL" && \
./scripts/create-sa-key.sh && \
export GOOGLE_APPLICATION_CREDENTIALS="$(pwd)/sa-key.json" && \
echo "Using bucket: $BUCKET_NAME"
unset GOOGLE_APPLICATION_CREDENTIALS
```

### 2) Local Test (DirectRunner)
```bash
docker compose build         
docker compose run pipeline \
  --project "$PROJECT_ID" \
  --output gs://$(terraform -chdir=terraform output -raw bucket_name)/output/shakespeare
```

### 3) Dataflow (DataflowRunner)
```bash
python pipeline.py \
  --runner               DataflowRunner \
  --project              "$PROJECT_ID" \
  --region               "$REGION" \
  --temp_location        "gs://$BUCKET_NAME/temp/" \
  --staging_location     "gs://$BUCKET_NAME/staging/" \
  --service_account_email "$DATAFLOW_SA_EMAIL" \
  --output               "gs://$BUCKET_NAME/output/shakespeare" \
  --max_num_workers      1
```

ğŸ‘‰Â Check **Dataflowâ€¯â†’â€¯Jobs** for progress, and **Cloudâ€¯Storage** for the Parquet file when done.

---

## ğŸ› ï¸Â Prerequisites

| Tool | Version | Purpose |
|------|---------|---------|
| **gcloud** | â‰¥â€¯467 | Auth & API enablement |
| **Terraform** | â‰¥â€¯1.3 | IaC provisioning |
| **DockerÂ &Â Compose** | â‰¥â€¯24 | Local DirectRunner (optional) |
| **Python** | â‰¥â€¯3.12 | Direct CLI submission |
| **Apache Beam (GCP)** | â‰¥â€¯2.65 | Transformation Engine |

Make sure the following APIs are **enabled**:

```bash
gcloud services enable dataflow.googleapis.com bigquery.googleapis.com bigquerystorage.googleapis.com compute.googleapis.com iam.googleapis.com serviceusage.googleapis.com storage.googleapis.com 
```

---

## â˜ï¸Â Infrastructure as Code â€“ Terraform

* **Randomized bucket**: `${project}-dataflow-temp-<suffix>` (multiâ€‘region **US**).  
* **Serviceâ€¯Account**: `dataflow-pipeline-sa@account.iam.gserviceaccount.com`  
  * `roles/dataflow.worker` â€¢ `roles/dataflow.developer` â€¢ `roles/storage.objectAdmin` (bucketâ€‘scoped)  
* **force_destroy** = true â†’ easy teardown.

---

## ğŸƒâ€â™‚ï¸Â Local Run (DirectRunner)

1. **Build**: `docker compose build`
2. **Run**:  
   ```bash
   docker compose build         
   docker compose run pipeline \
   --project "$PROJECT_ID" \
   --output gs://$(terraform -chdir=terraform output -raw bucket_name)/output/shakespeare
   ```
3. **Validate**: `gsutil ls gs://${BUCKET_NAME}/output/`

No Dataflow charges; only a tiny BigQuery read (under free tier).

---

## â˜ï¸Â Cloud Run (Dataflow)

Submit with either **Python** (native) or **Docker**:

```bash
# native
python pipeline.py \
  --runner               DataflowRunner \
  --project              "$PROJECT_ID" \
  --region               "$REGION" \
  --temp_location        "gs://$BUCKET_NAME/temp/" \
  --staging_location     "gs://$BUCKET_NAME/staging/" \
  --service_account_email "$DATAFLOW_SA_EMAIL" \
  --output               "gs://$BUCKET_NAME/output/shakespeare" \
  --max_num_workers      1
```

### Cost GuardrailsÂ ğŸ’¸

| Flag | Value | Why |
|------|-------|-----|
| `--max_num_workers` | **1** | Prevent autoâ€‘scaling |
| `--worker_machine_type` | `e2-small` (optional) | Cheapest vCPU |
| `--disk_size_gb` | `20` (optional) | Minimal persistent disk |
| **Data size** | 6â€¯MB read | Fits in free tier |

A single run typically finishes in <â€¯5â€¯min and costs **â‰ˆâ€¯$0.02**.

---

## ğŸ§¹Â Cost & Cleanup

| Resource | Ongoing cost | Cleanup |
|----------|--------------|---------|
| **GCS bucket** | a few MB âŸ¶ ~\$0.00 | `terraform destroy` |
| **Service account** | free | destroy |
| **Dataflow job** | perâ€‘job | autoâ€‘deleted after 30â€¯days |

Always destroy the bucket if you no longer need the output files.

---

## ğŸ“‚Â Project Structure

```
.
â”œâ”€â”€ create-sa-key.sh        # Script to generate GCP service account key
â”œâ”€â”€ Dockerfile              # Beam + PyArrow container image
â”œâ”€â”€ docker-compose.yml      # Local DirectRunner harness
â”œâ”€â”€ pipeline.py             # Apache Beam pipeline definition
â”œâ”€â”€ auto_gcs_locations.py   # Auto-inject GCS temp/staging options
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ terraform/
â”‚   â””â”€â”€ main.tf             # Terraform config: GCP resources
â”œâ”€â”€ setup_environment.sh    # Script for quick setup with Terraform
â””â”€â”€ README.md               # You're here
```

---